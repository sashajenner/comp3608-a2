\section{Results and Discussion}

\subsection{Classifier Accuracy}
The canonical Na\"ive Bayes (NB)\nomenclature{NB}{Na\"ive Bayes} and Decision Tree (DT)\nomenclature{DT}{Decision Tree} classification algorithms were implemented with tie decisions resulting in a `yes' and are hereafter referred to as MyNB and MyDT respectively. 10-fold stratified cross validation was then performed on these algorithms and 12 other inbuilt Weka algorithms using the dataset described in section \ref{sec:data} after normalisation and discretisation for the numeric and nominal classification algorithms respectively.

Tables \ref{tab:acc:num} and \ref{tab:acc:nom} present all the resulting accuracy figures for each tested classification algorithm, shown in percentage (\%), using both the full dataset and the dataset after CFS.

\begin{table}[h!]
    \caption{The 10-fold stratified cross validation accuracy in percentage (\%) of each tested \textit{numeric} classification algorithm using the dataset with and without CFS. \label{tab:acc:num}}
    \begin{center}
    \begin{tabular}{|m{2cm}*{8}{|c}|}
        \hline
        \textbf{Numeric Data} & ZeroR & 1R & 1NN &5NN &NB &MLP &SVM & \color{blue}MyNB \\
        \hline
        No feature selection & 65.1042 &70.8333 &67.8385 &74.4792 &75.1302 &75.3906 &76.3021 &75.2614 \\
        \hline
        CFS & 65.1042 &70.8333 &69.0104 &74.4792 &76.3021 &75.7813&76.6927 & 76.0407 \\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[h!]
    \caption{The 10-fold stratified cross validation accuracy in percentage (\%) of each tested \textit{nominal} classification algorithm using the dataset with and without CFS. \label{tab:acc:nom}}
    \begin{center}
    \begin{tabular}{|m{2cm}*{6}{|c}|}
        \hline
        \textbf{Nominal Data} & DT unpruned &DT pruned &\color{blue}MyDT &Bagg &Boost &RF \\
        \hline
        No feature selection &75 & 75.3906 &73.4484 &74.8698&76.1719&73.1771 \\
        \hline
        CFS & 79.4271 & 79.4271 &78.3869 & 78.5156 & 78.6458 & 78.9063 \\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\subsection{DT Diagrams}
Decision trees were built on the full discretised dataset using three different algorithms: MyDT, and two DT classifiers from Weka (DT unpruned and DT pruned). The two Weka variants were built using J48 (an implementation of the C4.5 algorithm) with default parameters, but differ in that one has been pruned in addition to the other \cite{weka}. The DT diagrams are displayed in Figures \ref{fig:mydt}, \ref{fig:dt_unprune} and \ref{fig:dt_prune}.

\begin{figure}[h!]
    \begin{obeylines}
        \input{DT_unpruned}
    \end{obeylines}
    \caption{The DT diagram of the Weka J48 algorithm \textit{without} pruning and trained on the full discretised dataset.\label{fig:dt_unprune}}
\end{figure}

\begin{figure}[h!]
    \begin{obeylines}
        \input{DT_pruned}
    \end{obeylines}
    \caption{The DT diagram of the Weka J48 algorithm \textit{with} pruning and trained on the full discretised dataset.\label{fig:dt_prune}}
\end{figure}

\subsection{Discussion}

\subsubsection{Feature Selection}

\subsubsection{Comparison of Classifiers}
