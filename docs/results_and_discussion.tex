\section{Results and Discussion}
% do they want p-values and stuff? like does that make sense in this context? i.e. is this binary predictor better than this one.\


\subsection{Classifier Accuracy}
The canonical Na\"ive Bayes (NB)\nomenclature{NB}{Na\"ive Bayes} and Decision Tree (DT)\nomenclature{DT}{Decision Tree} classification algorithms were implemented with tie decisions resulting in a `yes' and are hereafter referred to as MyNB and MyDT respectively. 10-fold stratified cross validation was then performed on these algorithms and 12 other inbuilt Weka algorithms using the dataset described in section \ref{sec:data} after normalisation and discretisation for the numeric and nominal classification algorithms respectively.

Tables \ref{tab:acc:num} and \ref{tab:acc:nom} present all the resulting accuracy figures for each tested classification algorithm, shown in percentage (\%) to 4 d.p.\nomenclature{d.p. decimal points} , using both the full dataset and the dataset after CFS, and coloured for ease of comparison.

\begin{table}[h]
    \caption{The 10-fold stratified cross validation accuracy in percentage (\%) of each tested \textit{numeric} classification algorithm using the dataset with and without CFS. \label{tab:acc:num}}
    \begin{center}
    \begin{tabular}{|m{2cm}*{8}{|R}|}
        \hline
        \textbf{Numeric Data} & ZeroR\nomenclature{ZeroR}{Classifier that always predicts the majority class} & 1R\nomenclature{1R}{One Rule} & 1NN\nomenclature{kNN}{k-Nearest Neighbours} &5NN &NB &MLP\nomenclature{MLP}{Multilayer Perceptron} &SVM\nomenclature{SVM}{Support Vector Machines} & \color{blue}MyNB \EndTableHeader \\
        \hline
        No feature selection & 65.1042 &70.8333 &67.8385 &74.4792 &75.1302 &75.3906 &76.3021 &75.2614 \\
        \hline
        CFS & 65.1042 &70.8333 &69.0104 &74.4792 &76.3021 &75.7813&76.6927 & 76.0407 \\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[h]
    \caption{The 10-fold stratified cross validation accuracy in percentage (\%) of each tested \textit{nominal} classification algorithm using the dataset with and without CFS. \label{tab:acc:nom}}
    \begin{center}
    \begin{tabular}{|m{2cm}*{6}{|R}|}
        \hline
        \textbf{Nominal Data} & DT unpruned &DT pruned &\color{blue}MyDT &Bagg\nomenclature{Bagg}{Bagging} &Boost\nomenclature{Boost}{Boosting} &RF\nomenclature{RF}{Random Forest} \EndTableHeader \\
        \hline
        No feature selection &75.0000 & 75.3906 &73.4484 &74.8698&76.1719&73.1771 \\
        \hline
        CFS & 79.4271 & 79.4271 &78.3869 & 78.5156 & 78.6458 & 78.9063 \\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\subsection{DT Diagrams}
Decision trees were built on the full discretised dataset using three different algorithms: MyDT, and two DT classifiers from Weka (DT unpruned and DT pruned). The MyDT tree was built using the ID3 algorithm (without pruning), which recursively builds a tree based on maximum information gain. The two Weka variants were built using J48 (an implementation of the C4.5 algorithm) with default parameters, but differ in that one has been pruned in addition to the other \cite{weka}. The DT diagrams are displayed in Figures \ref{fig:mydt}, \ref{fig:dt_unprune} and \ref{fig:dt_prune} in section \ref{sec:dix}.

\subsection{Discussion}
% kinda tempted to change structure? like combine DT diagrams with DT here idk

\subsubsection{Comparison of Classifiers}
% for that extra WOW we could have a bar chart or something?

% In the discussion, compare the performance of the classifiers, with and without feature selection. Compare your implementations of NB and DT with Weka’s.

Overall, the accuracy of the 14 classifiers ranged roughly between 65\% and 80\% with a mean of $\sim$74.5\%. \\

The best performing numeric classifier was the SVM, both with and without feature selection, where it achieved an accuracy of $\sim$76.7\% and $\sim$76.3\% respectively. Similar in performance were MyNB, NB and MLP, with accuracies roughly within 1\% of the SVM. This small difference in accuracies ranging from 75\% to 77\% is not necessarily indicative of algorithmic superiority but may be the effect of random noise in the testing dataset.

On the other hand, the worst performing numeric classifiers were ZeroR, 1R and 1NN, achieving accuracies between 65\% and 71\%. These simple algorithms are clearly not complex enough to capture patterns in the data, but are instead good points for comparison as to what is easily achievable (for example by predicting the majority class in ZeroR).

Within the nominal classifiers, the highest accuracy was $\sim$79.4\%, and was obtained by both the pruned and unpruned DT using feature selection. Despite this, all of the nominal classifiers performed well using feature selection, with accuracies ranging roughly between 78\% and 79.5\%. Without feature selection, the best performing nominal classifier was Boost with an accuracy of $\sim$76.2\%.

The worst performing nominal classifier was MyDT, with and without feature selection, where it achieved an accuracy of $\sim$78.3\% and $\sim$73.4\% respectively.
\\

The 6 nominal classifiers clearly performed much better than the 8 numeric ones with a mean accuracy of $\sim$76.8\% compared to $\sim$72.8\%. In addition, using CFS improved or equalled the performance of every classifier, with an average improvement in accuracy of $\sim$2.1\%. \\


The implementations of MyNB and Weka's NB only differ in terms of their running time performance. In fact, the minimal differences in accuracies evident in Table \ref{tab:acc:num} are most likely the result of different 10-fold data stratifications used in the cross validation accuracy calculations. On the other hand, the implementations of MyDT and Weka's two DTs differ profoundly. MyDT is built using the ID3 algorithm without pruning, whilst Weka uses J48 (an implementation of the 8th revision of the C4.5 algorithm \cite{weka}) which is very similar to ID3 but using the normalised information gain ratio as its splitting criterion. This resulted in Weka's two DTs performing better than MyDT with and without feature selection.


\subsubsection{Feature Selection}

% Discuss the effect of the feature selection – did CFS select a subset of the original features, and if so, did the selected subset make intuitive sense to you? Was the feature selection beneficial, i.e. did it improve accuracy, or have any other advantages? Why do you think this is the case?

% i listed them again here bc i kinda need to, already seems to be done in 2.1.
% is there a reasonable order i can use? like the metric itself
The feature selection method (CFS) selected a subset of five features from the original nine. They were, in no particular order:

\begin{itemize}
    \item Glucose
    \item Insulin
    \item BMI
    \item Pedigree
    \item Age
\end{itemize}

As the aim of CFS is to create a subset of features with high correlation to the class but low correlation to each other, we analyse whether the included and excluded features are as expected.

Features relating to glucose concentration and insulin have been found to be one of the strongest predictors of future diabetes in numerous other studies containing a wide range of predictors [a][b][c][d]. Although a high (negative) correlation between plasma glucose concentration and insulin levels has been demonstrated [e] which should reduce the likelihood of selected both features in CFS, it seems their correlation with diabetes is strong enough to have both features included. 

Similarly, BMI and other weight related features have been found to be a good predictor of future diabetes [a][c][d]. Within our dataset, both BMI and Triceps skin fold thickness are weight-related features and have been shown to have significant mutual correlation [f], which could explain why CFS only included one of these features (BMI). Comparatively, BMI has been found to have higher association with health-related risk factors than triceps skin fold thickness [g][h], which may explain why it was prioritised by CFS as a predictor for diabetes.

The other features selected by CFS have also been found to have high correlation with diabetes, which also remaining relatively independent from other risk factors. In particular, age has been show to be both a strong predictor [i] and also relatively uncorrelated with other features, as demonstrated by the inclusion of age in PCA on the same dataset [c]. Family history of diabetes has also been shown to be a relatively strong and independent predictor of diabetes, explaining its inclusion in CFS [j].


[a] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2551654/
this one says best predictors are closely related to glucose and BMI from a selection of 21

[b] https://care.diabetesjournals.org/content/30/6/1544
this one says insulin is significant

[c] https://www.sciencedirect.com/science/article/pii/S2352914819300176
this is literally the same as us. same dataset, they used PCA-based method, which is just for uncorrelation i think. got Glucose, BMI, blood pressure, age

[d] https://bmcendocrdisord.biomedcentral.com/articles/10.1186/s12902-019-0436-6
dataset also had age, blood pressure. selected glucose, bmi with other stuff. other factors might be better, but it at least didn't select blood pressure as expected

[e] https://pubmed.ncbi.nlm.nih.gov/12919921/
negative coreraltion between glucose nad insulin

[f] https://www.ijcmph.com/index.php/ijcmph/article/view/6059
high correlation bmi to skin fold

[g] https://files.eric.ed.gov/fulltext/EJ1201447.pdf
[h] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2697002/
both show BMI is more related to heath risk factors than skin fold thickness, even after conjtrolling for a numebr of hter factors. small but statistically significant difference

[i] https://www.sciencedirect.com/science/article/abs/pii/S1871402116303009

[j] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4038917/




% then show it was better / other advantages (speed, memory). state why.

Across all models, when training using features selected by CFS the accuracy was always at least as good as no feature selection. This affect was most apparent with tree-based models, with the accuracy increasing ~5\% in some cases (i.e. DT unpruned, MyDT, RF). This is likely due to a reduction in over fitting, stemming from the removal of features that did not add much information as they were already highly correlated with other features - a key component of CFS. When highly correlated features were included, the algorithms would use this extra feature to fit to noise in the data.

Another advantage of CFS is the reduction in data size with little or no decrease in performance. This can be especially powerful in pruning large datasets that have many features which are highly correlated with each other  or poorly correlated with the target classification. In our dataset for example, the number of features was reduced by over 40\%. Not only does this allow for smaller file sizes and therefore faster training, but it can also increase the interpretability of models as there are less features being used (e.g. when visualising decision trees).



\subsubsection{Decision Trees}

% How does your DT classifier compare with the unpruned and pruned DT generated by Weka? Discuss the role and effect of pruning.

% Comparison between the DT classifiers and discussion of pruning



- similarity: glucose was used as first split for all trees, second split level is also similar
- difference: much larger than equivalent unpruned, also less accurate suggesting overfitting
- then segway into generic desc of pruning. how it works, how it leads to shorter tree and still has more accuracy



\subsubsection{Tree-based Classifiers}

% Comparison between the tree-based classifiers
% Compare the accuracy of the tree-based classifiers (DT, Bagging, Boosting and RF).

how is this different from overall comparison of classifiers? basically just a comparison of nominal stuff
dont wanna overlap too much
i guess overall focuses on nominal vs numeric, and looks at best / worst overall

which DT method was used for dagg/boost/rf?


- boosting good even without CFS. try to speculate why. literature? is there a clear link between algos? boosting creates an iterative ensemble(?) of trees that focus on rows that we failed to predict, this is similar to having a number of uncorrelated features/trees QED? and then once CFS is used this advantage goes away
- RF bad? if this uses very short trees we can blame this on inability to capture complexity similar to numeric data.
- read literature about DFS J48 to figure out why its much better than other algos / MyDT. prob just generically list the "improvments" over ID3 and just go therefore it performs better.
- similar to bagging? bagging good bc reduces overfitting therefore good. if using small trees => still not able to fully capture complexity, therefore not as good as full J48 but better than RF. what tree does this use? if it uses full ID3 trees then this doesn't hold since its worse than them.


\subsubsection{?Anything else that we consider important}
% Include anything else that you consider important

Nominal better? Weird?? Discussion point?
or is the data being predicted here actually different? if not its probs just overfitting (to noise) or something when given more DOF(?) and thats something to mention.


could also talk about why J48 DT is the best. again, look into specifics of J48 and try to justify that it had all advantages of DT without disadvantages (+ advantages that other algos had).


if ur reading this then im already dead. jks im super busy until like 7pm today so ill turn these into actual paras when i get back.
